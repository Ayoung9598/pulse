### Big data architecture best practices
The preceding diagram depicts an end-to-end data pipeline in a data lake architecture using the AWS cloud platform with the following components:

![[Screenshot 2023-10-09 095510.png]]

-  AWS Direct Connect to set up a high-speed network connection between the on-premises data center and AWS to migrate data. If you have large volumes of archive data, it's better to use the AWS Snow family to move it offline. 
-  A data ingestion layer with various components to ingest streaming data using Amazon Kinesis, relational data using AWS Data Migration Service (DMS), secure file transfer using AWS Transfer for SFTP, and AWS DataSync to update data files between cloud and on-prem systems. • A centralized data storage for all data using Amazon S3, where data storage has multiple layers to store raw data, processed data, and archive data.
- A cloud native data warehouse solution, Amazon Redshift, with Redshift Spectrum to support lakehouse architecture. 
- An ad hoc query functionality using Amazon Athena. 
- A quick ETL pipeline based on Spark using AWS Glue. 
- Amazon EMR to re-utilize existing Hadoop scripts and other Apache Hadoop frameworks. • Amazon Lake Formation to build comprehensive data cataloging and granular access control at the data lake level. 
- The AI/ML extension with Amazon SageMaker.


